import torch
import clip
from PIL import Image

# Ref
# https://github.com/openai/CLIP
# https://simonezz.tistory.com/88

device = "cuda:0" if torch.cuda.is_available() else "cpu"

print("--- device ---")
print(device)

print("--- clip model list ---")
print(clip.available_models())
# ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']

model, preprocess = clip.load("ViT-B/32", device=device)

print("--- clip ---")
image = (
    preprocess(Image.open("learning_space/data/clip-test-image/CLIP.png"))
    .unsqueeze(0)
    .to(device)
)

word_list = ["a diagram", "a dog", "a cat"]
text = clip.tokenize(word_list).to(device)

print(image)
# tensor([[[[-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],
#       [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],
#       [-1.7923, -1.7923, -1.7923,  ..., -1.7923, -1.7923, -1.7923],
#       ...,
#       [ 1.9303,  1.9303,  1.9303,  ..., -1.7923, -1.7923, -1.7923],
#       [ 1.9303,  1.9303,  1.9303,  ..., -1.7923, -1.7923, -1.7923],
#       [ 1.0982,  1.0982,  1.0982,  ..., -1.7923, -1.7923, -1.7923]],

#      [[-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],
#       [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],
#       [-1.7521, -1.7521, -1.7521,  ..., -1.7521, -1.7521, -1.7521],
#       ...,
#       [ 2.0749,  2.0749,  2.0749,  ..., -1.7521, -1.7521, -1.7521],
#       [ 2.0749,  2.0749,  2.0749,  ..., -1.7521, -1.7521, -1.7521],
#       [ 1.2194,  1.2194,  1.2194,  ..., -1.7521, -1.7521, -1.7521]],

#      [[-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],
#       [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],
#       [-1.4802, -1.4802, -1.4802,  ..., -1.4802, -1.4802, -1.4802],
#       ...,
#       [ 2.1459,  2.1459,  2.1459,  ..., -1.4802, -1.4802, -1.4802],
#       [ 2.1459,  2.1459,  2.1459,  ..., -1.4802, -1.4802, -1.4802],
#       [ 1.3354,  1.3354,  1.3354,  ..., -1.4802, -1.4802, -1.4802]]]],
#    device='cuda:0')

print(text)
# tensor([[49406,   320, 22697, 49407,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0],
#         [49406,   320,  1929, 49407,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0],
#         [49406,   320,  2368, 49407,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
#              0,     0,     0,     0,     0,     0,     0]], device='cuda:0',
#        dtype=torch.int32)

with torch.no_grad():
    image_features = model.encode_image(image)
    print("image_features: ", image_features)
    # tensor([[ 3.1348e-01, -1.4539e-01,  3.0273e-01, -1.9946e-01,  1.4732e-02,
    #          -1.0406e-01,  1.5491e-01,  2.9468e-01,  4.7998e-01, -7.2632e-03,
    #          -3.1567e-01,  1.2891e-01, -1.3782e-01,  5.4785e-01,  2.1338e-01,
    #          -3.0347e-01, -1.5552e-01,  5.4150e-01, -4.2261e-01,  1.9385e-01,
    #           2.3010e-02, -1.2378e-01,  1.5125e-01,  8.3008e-02, -5.9180e-01,
    #           5.9131e-01, -1.4478e-01,  2.0850e-01,  1.9474e-03, -2.8003e-01,
    #           2.5513e-01,  2.3816e-01, -1.4954e-01,  7.6318e-01,  2.7026e-01,
    #           2.9358e-02,  1.5686e-01,  4.3091e-01,  1.0063e-02, -2.1387e+00,
    #          -6.1829e-02,  3.6890e-01, -2.6074e-01, -3.4131e-01, -1.7041e-01,
    #          -7.1338e-01, -3.4302e-01, -5.0934e-02, -3.2397e-01, -1.1890e-01,
    #           6.6040e-02, -2.5439e-01, -1.0809e-01,  5.8447e-01,  7.9712e-02,
    #          -2.9590e-01, -2.1164e-02,  4.7632e-01, -9.6130e-02,  5.6299e-01,
    #           6.9287e-01, -3.2318e-02,  2.3572e-01, -8.9539e-02,  2.1194e-02,
    #           6.5857e-02,  2.6318e-01,  7.1729e-01, -2.8174e-01, -1.7908e-01,
    #          -1.9336e-01, -2.6025e-01, -1.3733e-01,  3.9429e-01,  2.1228e-01,
    #          -5.2588e-01,  9.3811e-02, -5.5664e-02, -1.5955e-01, -3.5059e-01,
    #          -3.5059e-01,  3.1396e-01, -3.7817e-01,  5.9717e-01,  4.6112e-02,
    #           4.2529e-01,  1.2207e+00,  1.8286e-01,  5.6885e-01, -3.0884e-01,
    #           3.1909e-01, -4.0588e-02, -5.8672e+00,  4.5801e-01,  1.3757e-01,
    #          -2.2598e-02,  5.0293e-02, -3.0420e-01, -1.7322e-01, -2.9068e-02,
    #          -1.4758e-01, -3.3474e-03, -3.0957e-01, -2.2205e-01,  6.5002e-02,
    #          -3.6450e-01, -3.4434e+00,  5.3564e-01,  7.4951e-02,  6.7749e-03,
    #           7.6843e-02, -5.1855e-01, -4.1626e-02, -1.3513e-01, -3.8550e-01,
    #           2.2070e-01,  1.0547e-01, -7.7637e-01,  2.9102e-01,  2.9102e-01,
    #          -7.7881e-02, -2.4219e-01, -9.7961e-02,  9.9243e-02,  1.6345e-01,
    #           2.7295e-01,  3.2471e-02, -7.2876e-02,  2.4487e-01,  2.6611e-01,
    #           6.3916e-01,  6.0516e-02,  4.2999e-02,  8.1445e-01, -3.6194e-02,
    #          -3.4729e-02, -3.1787e-01, -7.6709e-01, -1.2720e-01,  1.7065e-01,
    #           3.8306e-01,  4.6730e-03, -5.1318e-01, -3.0249e-01, -2.1680e-01,
    #           5.1367e-01, -1.7151e-01,  1.9116e-01, -1.0583e-01, -2.3413e-01,
    #           8.6182e-02,  2.4451e-01, -1.5723e-01, -6.1981e-02,  2.8854e-02,
    #           7.8979e-02, -2.8760e-01, -2.7563e-01,  8.4412e-02, -1.9943e-02,
    #           6.7822e-01, -1.8591e-01,  1.5466e-01, -4.0918e-01,  4.8438e-01,
    #          -2.0508e-01, -7.6180e-03,  1.0199e-01, -5.9479e-02, -7.1716e-02,
    #          -8.0933e-02, -1.2402e-01, -7.8613e-02, -4.4727e-01, -2.3828e-01,
    #           3.8965e-01, -7.5781e-01,  4.3823e-01, -4.4098e-02,  4.3848e-01,
    #          -1.3000e-01, -6.5723e-01, -4.8926e-01,  1.4084e-02, -1.1847e-01,
    #          -3.0713e-01,  1.8005e-01,  4.4775e-01, -3.6963e-01, -1.1438e-01,
    #          -1.6479e-01, -5.8960e-02,  3.4210e-02, -4.3945e-03, -5.2002e-01,
    #          -1.8787e-01,  1.2830e-01,  4.1089e-01, -2.6550e-02,  1.1284e-02,
    #           2.6245e-01, -1.7078e-01, -8.2245e-03, -6.0693e-01,  2.7856e-01,
    #          -4.8370e-02, -3.5962e-01,  1.4526e-01,  1.8555e-01,  1.1023e-01,
    #          -1.5454e-01, -5.0928e-01, -1.9775e-02,  1.6663e-01, -6.1890e-02,
    #          -1.8384e-01,  3.8940e-01,  4.6558e-01,  4.7455e-02, -2.8641e-02,
    #          -5.1613e-03,  6.1621e-01,  1.2549e-01,  4.5654e-02, -2.1082e-01,
    #          -3.9624e-01,  4.5239e-01, -1.5112e-01, -7.1533e-02,  3.5352e-01,
    #          -1.6638e-01,  4.2212e-01, -2.6581e-02,  3.4155e-01, -3.0420e-01,
    #          -3.8116e-02, -8.6121e-02, -4.7112e-03, -2.8394e-01, -1.5027e-01,
    #          -4.0771e-01, -1.8689e-01,  2.3425e-01,  3.8184e-01, -1.7310e-01,
    #           7.5928e-01,  2.6221e-01,  2.1558e-01,  1.9104e-01,  3.6157e-01,
    #           4.2328e-02, -3.8757e-02, -1.4612e-01,  1.0864e-01, -1.6150e-01,
    #           4.9707e-01, -3.5132e-01, -7.9834e-02, -1.2402e+00,  9.4299e-02,
    #          -2.3840e-01,  1.2169e-02,  2.3108e-01,  5.4053e-01, -4.5361e-01,
    #           4.7798e-03, -3.7817e-01, -6.6846e-01, -1.4771e-01,  8.3496e-02,
    #          -3.0615e-01, -1.1719e-01, -1.0455e-01,  2.4243e-01,  9.8328e-02,
    #           3.8892e-01, -5.7251e-02,  2.3181e-01, -3.3508e-02,  5.4590e-01,
    #           2.9297e-01, -2.0251e-01, -3.5327e-01, -9.8343e-03,  4.0894e-02,
    #           3.1567e-01, -8.8684e-02, -4.3152e-02,  2.2873e-02, -1.8909e-01,
    #           3.6499e-01,  7.6538e-02, -1.1169e-01,  8.3130e-02,  2.7783e-01,
    #           2.7856e-01,  2.4918e-02, -4.9658e-01, -1.1639e-01,  8.2111e-04,
    #           3.0127e-01, -9.4299e-02, -4.0479e-01,  3.0493e-01,  5.1270e-03,
    #          -2.8247e-01,  3.4277e-01,  1.5906e-01,  7.0898e-01,  5.6885e-01,
    #          -6.5723e-01, -5.3314e-02,  8.1396e-01,  1.8005e-01, -3.2153e-01,
    #          -2.6514e-01,  2.1582e-01,  1.7322e-01,  8.8074e-02,  6.6650e-02,
    #           7.5439e-01,  1.7432e+00, -1.6632e-02,  3.5815e-01, -7.8760e-01,
    #          -2.7710e-01, -4.0576e-01, -3.0566e-01,  5.6702e-02,  9.1858e-03,
    #           3.0786e-01,  5.6006e-01,  2.4951e-01, -3.2788e-01, -1.7566e-01,
    #           1.8738e-01,  8.0322e-02,  2.5586e-01, -7.0374e-02,  1.6187e-01,
    #           2.5360e-02, -5.4932e-01,  2.7930e-01, -2.6749e-02, -8.4351e-02,
    #          -1.8250e-01,  4.4128e-02, -1.5160e-02,  1.9775e-01,  2.6416e-01,
    #          -8.7769e-02, -2.7319e-01,  2.0657e-03,  1.9516e-02, -1.4050e-01,
    #           1.3623e-01,  2.4438e-01, -6.3525e-01, -2.4927e-01,  6.2207e-01,
    #          -1.6431e-01, -1.5173e-01,  2.6587e-01, -2.6538e-01, -8.2715e-01,
    #           4.8853e-01,  3.5034e-02, -8.3057e-01,  2.4597e-01,  3.8770e-01,
    #          -3.1177e-01, -1.9873e-01,  4.9219e-01,  9.5459e-02, -5.3406e-02,
    #           5.2681e-03, -8.0566e-02,  1.0632e-01, -4.2749e-01, -3.1567e-01,
    #          -1.3293e-01, -3.1470e-01, -2.8564e-01,  1.0681e-01,  1.2024e-02,
    #          -9.4360e-02,  8.2092e-02, -5.7080e-01,  3.6938e-01,  1.0303e+00,
    #          -5.1465e-01,  3.7012e-01,  1.8604e-01, -6.4270e-02,  3.1348e-01,
    #           5.6671e-02, -2.0593e-01, -5.1318e-01, -4.4995e-01, -1.5198e-01,
    #           3.6304e-01,  1.0815e-01, -2.1582e-01,  5.7739e-02,  1.3477e-01,
    #          -1.6187e-01, -4.7485e-01, -1.2077e-02,  8.8135e-01, -1.8774e-01,
    #           2.4817e-01, -3.3838e-01, -1.2352e-02,  3.6890e-01,  2.3120e-01,
    #          -1.1340e-01,  5.5762e-01, -2.4829e-01, -2.8076e-01, -1.1584e-01,
    #          -1.8884e-01, -4.6600e-02,  3.0933e-01, -4.1284e-01, -3.5449e-01,
    #           8.7646e-02,  1.0980e-01, -3.2373e-01, -1.4023e+00, -1.2756e-01,
    #           1.1487e-01,  1.8066e-01, -4.0771e-02, -4.5624e-02, -1.5723e-01,
    #          -2.8052e-01, -2.2852e-01, -2.8931e-01, -1.4978e-01, -3.0762e-01,
    #          -2.0544e-01,  1.6028e-01, -4.2017e-01,  7.1350e-02,  3.8452e-01,
    #          -4.3481e-01, -1.2610e-01,  1.1536e-01,  1.0333e-01,  5.8740e-01,
    #          -8.5986e-01, -3.8281e-01,  4.6143e-01, -3.8745e-01, -1.8433e-01,
    #           3.7231e-02, -7.1167e-02,  8.1604e-02, -2.0493e-02,  9.6191e-01,
    #           4.6069e-01, -2.5293e-01,  4.9658e-01, -2.6465e-01, -5.6104e-01,
    #           1.6345e-01,  1.9958e-02, -5.3772e-02,  4.4556e-02, -2.1667e-01,
    #           3.8794e-01,  3.4814e-01, -1.3184e-01, -2.1777e-01,  5.4138e-02,
    #           1.3708e-01,  2.4207e-01,  2.4365e-01,  3.0615e-01,  7.2388e-02,
    #           5.9235e-02, -3.6011e-01, -2.1423e-01,  1.4282e-01,  2.5684e-01,
    #          -2.7740e-02, -1.9067e-01,  3.2495e-01,  7.4348e-03,  5.9692e-02,
    #           2.2839e-01, -1.7136e-02,  2.8931e-01, -4.5093e-01,  6.7749e-02,
    #           5.7343e-02,  2.9590e-01, -8.5632e-02, -2.5604e-02, -2.7734e-01,
    #          -5.5908e-01, -3.4961e-01, -3.4033e-01,  5.2295e-01, -9.7656e-03,
    #           1.4069e-02, -2.6465e-01,  7.0312e-02, -4.2334e-01,  8.8818e-01,
    #          -2.5464e-01, -1.1395e-01]], device='cuda:0', dtype=torch.float16)

    text_features = model.encode_text(text)
    print("text features: ", text_features)
    # text features:  tensor([[ 0.0546, -0.0065,  0.0494,  ..., -0.6646, -0.1287, -0.4954],
    #     [ 0.1445,  0.0222, -0.2910,  ..., -0.4470, -0.3416,  0.1798],
    #     [ 0.1979, -0.2045, -0.1537,  ..., -0.4512, -0.5664,  0.0600]],
    #    device='cuda:0', dtype=torch.float16)

    logits_per_image, logits_per_text = model(image, text)
    print(logits_per_image)
    # tensor([[25.5312, 20.0938, 19.7500]], device='cuda:0', dtype=torch.float16)
    print(logits_per_text)
    # tensor([[25.5312],
    #     [20.0938],
    #     [19.7500]], device='cuda:0', dtype=torch.float16)
    probs = logits_per_image.softmax(dim=-1).cpu().numpy()

print("Label probs:", probs)
# [[0.9927937  0.00421068 0.00299572]]
