import torch
import torch.nn as nn
import torch.nn.functional as F

# Ref
# https://tutorials.pytorch.kr/recipes/recipes/defining_a_neural_network.html
# https://reniew.github.io/12/
# https://yjjo.tistory.com/8


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()

        # 첫번째 2D 합성곱 계층
        # 1개의 입력 채널(이미지)을 받아들이고, 사각 커널 사이즈가 3인 32개의 합성곱 특징들을 출력합니다.
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        # input channel 1 / output channel 32
        # input channel 1 - black-white image / input channel 3 - color image
        # Output_height = floor((Input_height - Kernel_height(convolution filter) + 2 * Padding) / Stride) + 1
        # Output_width = floor((Input_width - Kernel_width(convolution filter) + 2 * Padding) / Stride) + 1

        # 두번째 2D 합성곱 계층
        # 32개의 입력 계층을 받아들이고, 사각 커널 사이즈가 3인 64개의 합성곱 특징을 출력합니다.
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        # input channel 32 / output channel 64

        # 인접한 픽셀들은 입력 확률에 따라 모두 0 값을 가지거나 혹은 모두 유효한 값이 되도록 만듭니다.
        self.dropout1 = nn.Dropout2d(0.25)
        self.dropout2 = nn.Dropout2d(0.5)

        # 첫번째 fully connected layer
        self.fc1 = nn.Linear(9216, 128)
        # 10개의 라벨을 출력하는 두번째 fully connected layer
        self.fc2 = nn.Linear(128, 10)

    # x는 데이터를 나타냅니다.
    def forward(self, x):
        print("Input data")
        print(x)
        print(x.size())

        # 데이터가 conv1을 지나갑니다.
        print(f"Pass the conv1:{self.conv1}")
        x = self.conv1(x)
        print(x)

        # x를 ReLU 활성함수(rectified-linear activation function)에 대입합니다.
        print("Activation Function: ReLU")
        x = F.relu(x)
        print(x)

        print(f"Pass the conv1:{self.conv2}")
        x = self.conv2(x)
        print(x)

        print("Activation Function: sigmoid")
        x = F.sigmoid(x)
        print(x)

        # x에 대해서 max pooling을 실행합니다.
        print("Max pool2d")
        x = F.max_pool2d(x, 2)
        print(x)

        # 데이터가 dropout1을 지나갑니다.
        print("Pass the dropout1")
        x = self.dropout1(x)
        print(x)

        # start_dim=1으로 x를 압축합니다.
        print("Make flatten")
        x = torch.flatten(x, 1)
        print(x)

        # 데이터가 fc1을 지나갑니다.
        print("Pass the fc1")
        x = self.fc1(x)
        print(x)

        print("Activation Function: sigmoid")
        x = F.elu(x)
        print(x)

        print("Activation Function: leaky_relu")
        x = F.leaky_relu(x)
        print(x)

        print("Pass the fc2")
        x = self.fc2(x)
        print(x)

        # x에 softmax를 적용합니다.
        print("Activation Function: softmax")
        output = F.log_softmax(x, dim=1)
        print(x)
        return output


# 임의의 28x28 이미지로 맞춰줍니다.
random_data = torch.rand((1, 1, 28, 28))  # Sample, Channel, H, W
print(28 * 28)

my_nn = Net()
result = my_nn(random_data)
print(result)

# 784
# Input data
# tensor([[[[0.4089, 0.2831, 0.4892, 0.7520, 0.5609, 0.8925, 0.7510, 0.9500,
#            0.7826, 0.4913, 0.6563, 0.7672, 0.2370, 0.4315, 0.3880, 0.9964,
#            0.9773, 0.9407, 0.0522, 0.8607, 0.0325, 0.9421, 0.5712, 0.3638,
#            0.6527, 0.1700, 0.9445, 0.5778],
#           [0.3717, 0.2054, 0.0317, 0.8140, 0.0041, 0.2261, 0.0024, 0.2937,
#            0.4483, 0.3629, 0.7522, 0.2307, 0.0477, 0.9134, 0.6475, 0.2563,
#            0.6520, 0.9503, 0.8188, 0.6936, 0.9217, 0.4008, 0.2779, 0.2458,
#            0.3681, 0.6980, 0.2544, 0.2002],
#           [0.3159, 0.9402, 0.8376, 0.5731, 0.0610, 0.2440, 0.3081, 0.3708,
#            0.3113, 0.0128, 0.8840, 0.0974, 0.8383, 0.6636, 0.9437, 0.3472,
#            0.5191, 0.5914, 0.5961, 0.5917, 0.5188, 0.9557, 0.5152, 0.4602,
#            0.5821, 0.4236, 0.7355, 0.4436],
#           [0.1955, 0.6697, 0.5892, 0.1701, 0.6933, 0.8790, 0.5439, 0.2306,
#            0.5307, 0.1890, 0.4797, 0.2473, 0.5725, 0.2423, 0.8000, 0.4432,
#            0.9865, 0.4818, 0.1346, 0.7809, 0.0295, 0.2337, 0.3657, 0.5585,
#            0.8260, 0.2189, 0.5165, 0.8117],
#           [0.7342, 0.0135, 0.7034, 0.4363, 0.8313, 0.0309, 0.0494, 0.9193,
#            0.7601, 0.3132, 0.0408, 0.5411, 0.5743, 0.7811, 0.6315, 0.0284,
#            0.0027, 0.4830, 0.8118, 0.5456, 0.6736, 0.5469, 0.3798, 0.6503,
#            0.4095, 0.0213, 0.3678, 0.7807],
#           [0.4494, 0.5990, 0.5605, 0.1226, 0.5614, 0.7195, 0.1640, 0.9610,
#            0.5494, 0.0702, 0.1002, 0.0702, 0.5147, 0.9006, 0.3821, 0.8454,
#            0.5698, 0.2517, 0.7459, 0.8033, 0.8310, 0.2330, 0.7969, 0.5008,
#            0.4673, 0.0067, 0.4462, 0.0833],
#           [0.4134, 0.9692, 0.6206, 0.0284, 0.2597, 0.6022, 0.8069, 0.3461,
#            0.8492, 0.2825, 0.2671, 0.1114, 0.5368, 0.6968, 0.5660, 0.1691,
#            0.3920, 0.5597, 0.4117, 0.9271, 0.5042, 0.3249, 0.5849, 0.4617,
#            0.8656, 0.7432, 0.4878, 0.4523],
#           [0.9463, 0.4112, 0.8925, 0.2530, 0.1915, 0.2111, 0.5821, 0.3093,
#            0.1144, 0.0831, 0.4403, 0.1863, 0.1901, 0.9152, 0.8471, 0.0837,
#            0.6129, 0.1275, 0.1046, 0.2522, 0.1765, 0.5200, 0.4788, 0.6183,
#            0.2894, 0.3235, 0.7841, 0.3631],
#           [0.9758, 0.3543, 0.4692, 0.2939, 0.1896, 0.4486, 0.8668, 0.1944,
#            0.1454, 0.4249, 0.6929, 0.4358, 0.9424, 0.4524, 0.2376, 0.8842,
#            0.5637, 0.0165, 0.1269, 0.3647, 0.0469, 0.6617, 0.1930, 0.6696,
#            0.1576, 0.9964, 0.3512, 0.1729],
#           [0.7326, 0.5441, 0.9885, 0.2904, 0.7968, 0.4964, 0.4927, 0.6958,
#            0.0684, 0.2695, 0.9259, 0.1510, 0.6575, 0.6471, 0.8404, 0.8827,
#            0.8360, 0.7555, 0.4618, 0.4318, 0.3158, 0.6751, 0.4319, 0.7396,
#            0.8901, 0.6581, 0.2369, 0.5906],
#           [0.6340, 0.0117, 0.1715, 0.1630, 0.3993, 0.2868, 0.3390, 0.0592,
#            0.8114, 0.6488, 0.9567, 0.1426, 0.7725, 0.8933, 0.5987, 0.6842,
#            0.1758, 0.4955, 0.7368, 0.9256, 0.1661, 0.8030, 0.4497, 0.0557,
#            0.8976, 0.0036, 0.1068, 0.4074],
#           [0.4431, 0.1843, 0.1036, 0.9182, 0.7062, 0.8007, 0.5291, 0.1213,
#            0.5437, 0.3738, 0.2444, 0.9946, 0.2228, 0.8176, 0.6282, 0.6014,
#            0.2114, 0.1269, 0.2400, 0.1175, 0.5535, 0.6379, 0.4435, 0.6046,
#            0.5935, 0.8390, 0.5117, 0.8108],
#           [0.0127, 0.6608, 0.6593, 0.6563, 0.1090, 0.3829, 0.9783, 0.1103,
#            0.4541, 0.3055, 0.5849, 0.2971, 0.4787, 0.3203, 0.2771, 0.0620,
#            0.5688, 0.7609, 0.8787, 0.4799, 0.7539, 0.9735, 0.8567, 0.8718,
#            0.6168, 0.0035, 0.2221, 0.7730],
#           [0.8028, 0.4040, 0.9294, 0.8886, 0.4786, 0.3445, 0.4240, 0.9118,
#            0.1128, 0.7160, 0.6594, 0.4245, 0.6753, 0.5241, 0.8079, 0.7757,
#            0.8925, 0.7273, 0.2530, 0.9234, 0.7220, 0.6189, 0.1620, 0.1105,
#            0.8242, 0.8902, 0.8978, 0.8990],
#           [0.6825, 0.3767, 0.9534, 0.6701, 0.3394, 0.8728, 0.1951, 0.7532,
#            0.0666, 0.2708, 0.4307, 0.6283, 0.2148, 0.7109, 0.0120, 0.7734,
#            0.9006, 0.7897, 0.8865, 0.5064, 0.3740, 0.8314, 0.2402, 0.8709,
#            0.1761, 0.5313, 0.9803, 0.3434],
#           [0.1875, 0.0342, 0.5818, 0.7590, 0.0314, 0.9047, 0.9869, 0.4586,
#            0.7987, 0.0111, 0.1740, 0.0701, 0.3312, 0.5931, 0.0348, 0.1969,
#            0.7497, 0.0117, 0.5166, 0.5962, 0.8955, 0.3028, 0.4256, 0.3522,
#            0.5559, 0.0737, 0.1587, 0.9948],
#           [0.5143, 0.6381, 0.2525, 0.0780, 0.6752, 0.8650, 0.6550, 0.1212,
#            0.0393, 0.4908, 0.8070, 0.9682, 0.7723, 0.6275, 0.6144, 0.8033,
#            0.4011, 0.0087, 0.2031, 0.7315, 0.9491, 0.3215, 0.9328, 0.9252,
#            0.1590, 0.9088, 0.0976, 0.3507],
#           [0.7424, 0.7659, 0.8526, 0.2617, 0.5765, 0.0420, 0.4969, 0.5891,
#            0.3067, 0.3521, 0.0895, 0.3859, 0.9997, 0.0150, 0.5358, 0.3674,
#            0.2906, 0.8643, 0.5594, 0.6888, 0.2209, 0.3353, 0.1114, 0.3512,
#            0.6150, 0.8596, 0.8573, 0.9173],
#           [0.6177, 0.8821, 0.4962, 0.1671, 0.3942, 0.0046, 0.9520, 0.5447,
#            0.6510, 0.3188, 0.0498, 0.8159, 0.4516, 0.7864, 0.1735, 0.4788,
#            0.4748, 0.6003, 0.9688, 0.4636, 0.1179, 0.0412, 0.3881, 0.7674,
#            0.7317, 0.0471, 0.0379, 0.4851],
#           [0.5962, 0.9429, 0.4577, 0.8312, 0.4456, 0.0432, 0.3890, 0.1701,
#            0.9447, 0.7189, 0.9862, 0.5931, 0.5644, 0.2009, 0.7910, 0.1897,
#            0.6728, 0.9885, 0.9056, 0.6492, 0.7103, 0.7918, 0.0022, 0.6955,
#            0.2483, 0.8379, 0.1723, 0.7911],
#           [0.8802, 0.5796, 0.0811, 0.7362, 0.8166, 0.0241, 0.3672, 0.0700,
#            0.1876, 0.0298, 0.3754, 0.6783, 0.4133, 0.0467, 0.8767, 0.3787,
#            0.6025, 0.2463, 0.3491, 0.8924, 0.3673, 0.6099, 0.0064, 0.5828,
#            0.1089, 0.7512, 0.6202, 0.9939],
#           [0.3513, 0.7492, 0.9570, 0.1222, 0.4148, 0.8771, 0.8119, 0.2018,
#            0.1878, 0.0119, 0.6328, 0.8099, 0.2931, 0.3114, 0.2126, 0.3547,
#            0.1029, 0.8496, 0.8969, 0.0348, 0.9929, 0.4066, 0.8029, 0.2944,
#            0.1435, 0.8315, 0.5517, 0.7592],
#           [0.4414, 0.0887, 0.4798, 0.4292, 0.0175, 0.3675, 0.9499, 0.3783,
#            0.2601, 0.0581, 0.2434, 0.4672, 0.1357, 0.3098, 0.1667, 0.1464,
#            0.4158, 0.7951, 0.1581, 0.5663, 0.7191, 0.2539, 0.0769, 0.4840,
#            0.3885, 0.6325, 0.6581, 0.2137],
#           [0.5334, 0.2022, 0.4707, 0.0624, 0.7457, 0.7470, 0.7244, 0.9696,
#            0.0712, 0.3466, 0.2693, 0.4589, 0.1757, 0.3322, 0.9823, 0.3709,
#            0.7821, 0.0577, 0.3315, 0.4442, 0.9855, 0.1505, 0.8891, 0.8494,
#            0.1090, 0.5732, 0.3373, 0.0397],
#           [0.0461, 0.8027, 0.3726, 0.3157, 0.3360, 0.0773, 0.2344, 0.1981,
#            0.9993, 0.1274, 0.5904, 0.2489, 0.9186, 0.0634, 0.3952, 0.5869,
#            0.5682, 0.6018, 0.0909, 0.4224, 0.3571, 0.0648, 0.0146, 0.5795,
#            0.9943, 0.4935, 0.0718, 0.8942],
#           [0.6900, 0.0475, 0.8811, 0.9507, 0.6996, 0.0676, 0.5837, 0.3497,
#            0.5673, 0.3017, 0.6047, 0.6369, 0.1749, 0.4781, 0.3190, 0.3912,
#            0.0214, 0.1095, 0.0866, 0.3452, 0.4258, 0.9333, 0.8643, 0.8168,
#            0.5110, 0.1901, 0.4600, 0.0943],
#           [0.7433, 0.7488, 0.5460, 0.3329, 0.3908, 0.0637, 0.3805, 0.6187,
#            0.7557, 0.5493, 0.3818, 0.0089, 0.0223, 0.7549, 0.3348, 0.8075,
#            0.7174, 0.7134, 0.2447, 0.4000, 0.4397, 0.3541, 0.3796, 0.8960,
#            0.2740, 0.9629, 0.0090, 0.7445],
#           [0.2619, 0.6254, 0.5099, 0.4572, 0.2471, 0.6543, 0.1724, 0.7144,
#            0.3750, 0.5552, 0.4026, 0.6763, 0.9661, 0.8121, 0.9273, 0.2064,
#            0.8340, 0.5325, 0.5146, 0.9784, 0.1162, 0.1221, 0.2013, 0.1951,
#            0.9214, 0.8663, 0.5373, 0.5936]]]])
# torch.Size([1, 1, 28, 28])
# Pass the conv1:Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))
# tensor([[[[-0.1097, -0.0012,  0.1971,  ...,  0.0633, -0.2669,  0.1100],
#           [-0.0764, -0.0221,  0.2320,  ...,  0.0324,  0.0995, -0.1901],
#           [-0.0749, -0.1659, -0.0459,  ...,  0.1517, -0.1767, -0.2233],
#           ...,
#           [-0.1701, -0.0388, -0.0803,  ..., -0.0547,  0.1246, -0.0239],
#           [ 0.1524, -0.0474, -0.0009,  ...,  0.0366,  0.0348, -0.1663],
#           [-0.3148,  0.0500, -0.0514,  ..., -0.2007,  0.0824,  0.1660]],

#          [[-0.1649, -0.0418,  0.3398,  ..., -0.1577,  0.1256,  0.1311],
#           [-0.1505,  0.4827,  0.1683,  ...,  0.2387,  0.0397, -0.2054],
#           [ 0.1529, -0.0598, -0.3184,  ...,  0.3508,  0.0507, -0.2955],
#           ...,
#           [-0.0237, -0.2234,  0.0625,  ...,  0.2983,  0.2727, -0.3138],
#           [-0.0191, -0.0429,  0.2443,  ...,  0.1381, -0.2424,  0.1728],
#           [ 0.1100,  0.3946,  0.0372,  ..., -0.3930,  0.2888, -0.0328]],

#          [[ 0.4704,  0.1890,  0.1810,  ...,  0.2812,  0.3500,  0.4360],
#           [ 0.2773,  0.1597,  0.3172,  ...,  0.2365,  0.2157,  0.4411],
#           [ 0.2595,  0.6721,  0.4100,  ...,  0.2234,  0.3848,  0.4625],
#           ...,
#           [ 0.2116,  0.5398,  0.2543,  ...,  0.2032,  0.3250,  0.3178],
#           [ 0.2625,  0.1650,  0.2374,  ...,  0.4452,  0.4646,  0.1690],
#           [ 0.4237,  0.3265,  0.4780,  ...,  0.5968,  0.1954,  0.2337]],

#          ...,

#          [[-0.1509,  0.0885,  0.2592,  ...,  0.0776,  0.2157,  0.0406],
#           [-0.0202,  0.1454,  0.2445,  ...,  0.0163,  0.3504,  0.0814],
#           [ 0.4381,  0.0978,  0.2764,  ...,  0.2666,  0.2909,  0.0733],
#           ...,
#           [ 0.3242, -0.1785,  0.0333,  ...,  0.3305,  0.2085,  0.0094],
#           [ 0.0823,  0.2854,  0.2606,  ...,  0.4586, -0.0224,  0.3660],
#           [ 0.0851,  0.2032,  0.3059,  ...,  0.0070,  0.1447,  0.1603]],

#          [[ 0.2413,  0.0823,  0.2288,  ...,  0.2295,  0.3768,  0.2855],
#           [ 0.2594,  0.3024,  0.6050,  ...,  0.1374,  0.4642,  0.4508],
#           [ 0.3753,  0.4471,  0.5715,  ...,  0.3830,  0.4911,  0.2723],
#           ...,
#           [ 0.4429,  0.3843,  0.2151,  ...,  0.4403,  0.5490,  0.2539],
#           [ 0.2348,  0.2729,  0.4906,  ...,  0.7041,  0.2888,  0.4247],
#           [ 0.3540,  0.3031,  0.4171,  ...,  0.4562,  0.3231,  0.4535]],

#          [[-0.1569,  0.1707, -0.0296,  ...,  0.1681,  0.0329,  0.1157],
#           [ 0.1582,  0.3395,  0.3514,  ...,  0.1061,  0.3958,  0.0136],
#           [ 0.3472, -0.1630,  0.1449,  ...,  0.1669,  0.3293,  0.1778],
#           ...,
#           [ 0.1785,  0.0510, -0.0279,  ...,  0.1055,  0.3474,  0.2202],
#           [ 0.3612,  0.1208,  0.3802,  ...,  0.2013, -0.1681,  0.1826],
#           [ 0.0744,  0.2933,  0.0322,  ...,  0.0591, -0.0931,  0.4152]]]],
#        grad_fn=<ConvolutionBackward0>)
# Activation Function: ReLU
# tensor([[[[0.0000, 0.0000, 0.1971,  ..., 0.0633, 0.0000, 0.1100],
#           [0.0000, 0.0000, 0.2320,  ..., 0.0324, 0.0995, 0.0000],
#           [0.0000, 0.0000, 0.0000,  ..., 0.1517, 0.0000, 0.0000],
#           ...,
#           [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1246, 0.0000],
#           [0.1524, 0.0000, 0.0000,  ..., 0.0366, 0.0348, 0.0000],
#           [0.0000, 0.0500, 0.0000,  ..., 0.0000, 0.0824, 0.1660]],

#          [[0.0000, 0.0000, 0.3398,  ..., 0.0000, 0.1256, 0.1311],
#           [0.0000, 0.4827, 0.1683,  ..., 0.2387, 0.0397, 0.0000],
#           [0.1529, 0.0000, 0.0000,  ..., 0.3508, 0.0507, 0.0000],
#           ...,
#           [0.0000, 0.0000, 0.0625,  ..., 0.2983, 0.2727, 0.0000],
#           [0.0000, 0.0000, 0.2443,  ..., 0.1381, 0.0000, 0.1728],
#           [0.1100, 0.3946, 0.0372,  ..., 0.0000, 0.2888, 0.0000]],

#          [[0.4704, 0.1890, 0.1810,  ..., 0.2812, 0.3500, 0.4360],
#           [0.2773, 0.1597, 0.3172,  ..., 0.2365, 0.2157, 0.4411],
#           [0.2595, 0.6721, 0.4100,  ..., 0.2234, 0.3848, 0.4625],
#           ...,
#           [0.2116, 0.5398, 0.2543,  ..., 0.2032, 0.3250, 0.3178],
#           [0.2625, 0.1650, 0.2374,  ..., 0.4452, 0.4646, 0.1690],
#           [0.4237, 0.3265, 0.4780,  ..., 0.5968, 0.1954, 0.2337]],

#          ...,

#          [[0.0000, 0.0885, 0.2592,  ..., 0.0776, 0.2157, 0.0406],
#           [0.0000, 0.1454, 0.2445,  ..., 0.0163, 0.3504, 0.0814],
#           [0.4381, 0.0978, 0.2764,  ..., 0.2666, 0.2909, 0.0733],
#           ...,
#           [0.3242, 0.0000, 0.0333,  ..., 0.3305, 0.2085, 0.0094],
#           [0.0823, 0.2854, 0.2606,  ..., 0.4586, 0.0000, 0.3660],
#           [0.0851, 0.2032, 0.3059,  ..., 0.0070, 0.1447, 0.1603]],

#          [[0.2413, 0.0823, 0.2288,  ..., 0.2295, 0.3768, 0.2855],
#           [0.2594, 0.3024, 0.6050,  ..., 0.1374, 0.4642, 0.4508],
#           [0.3753, 0.4471, 0.5715,  ..., 0.3830, 0.4911, 0.2723],
#           ...,
#           [0.4429, 0.3843, 0.2151,  ..., 0.4403, 0.5490, 0.2539],
#           [0.2348, 0.2729, 0.4906,  ..., 0.7041, 0.2888, 0.4247],
#           [0.3540, 0.3031, 0.4171,  ..., 0.4562, 0.3231, 0.4535]],

#          [[0.0000, 0.1707, 0.0000,  ..., 0.1681, 0.0329, 0.1157],
#           [0.1582, 0.3395, 0.3514,  ..., 0.1061, 0.3958, 0.0136],
#           [0.3472, 0.0000, 0.1449,  ..., 0.1669, 0.3293, 0.1778],
#           ...,
#           [0.1785, 0.0510, 0.0000,  ..., 0.1055, 0.3474, 0.2202],
#           [0.3612, 0.1208, 0.3802,  ..., 0.2013, 0.0000, 0.1826],
#           [0.0744, 0.2933, 0.0322,  ..., 0.0591, 0.0000, 0.4152]]]],
#        grad_fn=<ReluBackward0>)
# Pass the conv1:Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))
# tensor([[[[ 0.0094,  0.1540,  0.0891,  ...,  0.0734,  0.0499,  0.1317],
#           [ 0.1324, -0.0248,  0.0689,  ...,  0.0144,  0.1410,  0.0599],
#           [ 0.2345,  0.0641,  0.0644,  ...,  0.1100,  0.1411,  0.1435],
#           ...,
#           [ 0.1243,  0.1616,  0.1188,  ...,  0.0366,  0.0081,  0.1863],
#           [ 0.0144, -0.0110,  0.1823,  ...,  0.0585,  0.0890,  0.1053],
#           [ 0.0978,  0.0509,  0.0767,  ...,  0.0026,  0.1713,  0.1022]],

#          [[ 0.3839,  0.3628,  0.3446,  ...,  0.4192,  0.3256,  0.3958],
#           [ 0.1982,  0.4004,  0.3155,  ...,  0.2796,  0.3229,  0.3080],
#           [ 0.3616,  0.4072,  0.3493,  ...,  0.4354,  0.3321,  0.3145],
#           ...,
#           [ 0.4716,  0.3118,  0.3005,  ...,  0.3587,  0.3561,  0.3484],
#           [ 0.3118,  0.3964,  0.3144,  ...,  0.3505,  0.2573,  0.4035],
#           [ 0.3375,  0.3424,  0.2878,  ...,  0.3456,  0.4695,  0.2912]],

#          [[-0.1521,  0.0123, -0.0445,  ..., -0.0207, -0.0593,  0.0109],
#           [ 0.0283, -0.0664, -0.0045,  ..., -0.0514,  0.0068, -0.0349],
#           [ 0.0662, -0.0683, -0.0432,  ..., -0.0763, -0.1115, -0.0316],
#           ...,
#           [-0.0497, -0.1112, -0.0539,  ..., -0.0363, -0.0595, -0.0208],
#           [-0.0166, -0.0184, -0.0714,  ...,  0.0081, -0.0192, -0.0573],
#           [-0.0138, -0.0312, -0.0070,  ..., -0.0020, -0.0431, -0.0666]],

#          ...,

#          [[-0.0888, -0.1415, -0.0606,  ..., -0.1017, -0.0840, -0.1518],
#           [-0.1133, -0.0387, -0.0581,  ..., -0.0697, -0.1026, -0.1660],
#           [-0.1077, -0.2074, -0.1276,  ..., -0.0986, -0.1246, -0.1186],
#           ...,
#           [-0.1436, -0.1013, -0.0937,  ..., -0.1454, -0.0711, -0.0913],
#           [-0.1282, -0.0782, -0.1130,  ..., -0.0920, -0.1068, -0.1339],
#           [-0.1031, -0.1065, -0.0620,  ..., -0.1019, -0.1472, -0.0746]],

#          [[ 0.0799, -0.1166, -0.0059,  ...,  0.0293,  0.0229, -0.0225],
#           [-0.1072,  0.0690,  0.0604,  ..., -0.0334, -0.0799,  0.0214],
#           [-0.0060,  0.0702, -0.0759,  ..., -0.0160, -0.1078, -0.0646],
#           ...,
#           [-0.0239, -0.0556, -0.0516,  ..., -0.0739,  0.0037, -0.0276],
#           [-0.0381,  0.0387, -0.0748,  ..., -0.0697, -0.0733, -0.1156],
#           [ 0.0303, -0.0239, -0.0404,  ...,  0.0541, -0.1102,  0.0137]],

#          [[ 0.3423,  0.3389,  0.3812,  ...,  0.3579,  0.2781,  0.3604],
#           [ 0.3892,  0.3375,  0.3299,  ...,  0.3157,  0.3010,  0.3662],
#           [ 0.2719,  0.4137,  0.3164,  ...,  0.3115,  0.3607,  0.3515],
#           ...,
#           [ 0.3044,  0.4023,  0.3397,  ...,  0.3791,  0.3669,  0.3327],
#           [ 0.3978,  0.3004,  0.2412,  ...,  0.3921,  0.3620,  0.3883],
#           [ 0.3580,  0.3532,  0.3372,  ...,  0.3273,  0.4026,  0.3615]]]],
#        grad_fn=<ConvolutionBackward0>)
# Activation Function: sigmoid
# tensor([[[[0.5023, 0.5384, 0.5223,  ..., 0.5183, 0.5125, 0.5329],
#           [0.5331, 0.4938, 0.5172,  ..., 0.5036, 0.5352, 0.5150],
#           [0.5584, 0.5160, 0.5161,  ..., 0.5275, 0.5352, 0.5358],
#           ...,
#           [0.5310, 0.5403, 0.5297,  ..., 0.5091, 0.5020, 0.5465],
#           [0.5036, 0.4972, 0.5454,  ..., 0.5146, 0.5222, 0.5263],
#           [0.5244, 0.5127, 0.5192,  ..., 0.5007, 0.5427, 0.5255]],

#          [[0.5948, 0.5897, 0.5853,  ..., 0.6033, 0.5807, 0.5977],
#           [0.5494, 0.5988, 0.5782,  ..., 0.5694, 0.5800, 0.5764],
#           [0.5894, 0.6004, 0.5864,  ..., 0.6072, 0.5823, 0.5780],
#           ...,
#           [0.6158, 0.5773, 0.5746,  ..., 0.5887, 0.5881, 0.5862],
#           [0.5773, 0.5978, 0.5780,  ..., 0.5867, 0.5640, 0.5995],
#           [0.5836, 0.5848, 0.5714,  ..., 0.5856, 0.6153, 0.5723]],

#          [[0.4620, 0.5031, 0.4889,  ..., 0.4948, 0.4852, 0.5027],
#           [0.5071, 0.4834, 0.4989,  ..., 0.4871, 0.5017, 0.4913],
#           [0.5165, 0.4829, 0.4892,  ..., 0.4809, 0.4722, 0.4921],
#           ...,
#           [0.4876, 0.4722, 0.4865,  ..., 0.4909, 0.4851, 0.4948],
#           [0.4958, 0.4954, 0.4822,  ..., 0.5020, 0.4952, 0.4857],
#           [0.4965, 0.4922, 0.4982,  ..., 0.4995, 0.4892, 0.4833]],

#          ...,

#          [[0.4778, 0.4647, 0.4849,  ..., 0.4746, 0.4790, 0.4621],
#           [0.4717, 0.4903, 0.4855,  ..., 0.4826, 0.4744, 0.4586],
#           [0.4731, 0.4483, 0.4681,  ..., 0.4754, 0.4689, 0.4704],
#           ...,
#           [0.4642, 0.4747, 0.4766,  ..., 0.4637, 0.4822, 0.4772],
#           [0.4680, 0.4805, 0.4718,  ..., 0.4770, 0.4733, 0.4666],
#           [0.4742, 0.4734, 0.4845,  ..., 0.4745, 0.4633, 0.4814]],

#          [[0.5200, 0.4709, 0.4985,  ..., 0.5073, 0.5057, 0.4944],
#           [0.4732, 0.5172, 0.5151,  ..., 0.4916, 0.4800, 0.5053],
#           [0.4985, 0.5175, 0.4810,  ..., 0.4960, 0.4731, 0.4838],
#           ...,
#           [0.4940, 0.4861, 0.4871,  ..., 0.4815, 0.5009, 0.4931],
#           [0.4905, 0.5097, 0.4813,  ..., 0.4826, 0.4817, 0.4711],
#           [0.5076, 0.4940, 0.4899,  ..., 0.5135, 0.4725, 0.5034]],

#          [[0.5848, 0.5839, 0.5942,  ..., 0.5885, 0.5691, 0.5891],
#           [0.5961, 0.5836, 0.5817,  ..., 0.5783, 0.5747, 0.5905],
#           [0.5676, 0.6020, 0.5785,  ..., 0.5773, 0.5892, 0.5870],
#           ...,
#           [0.5755, 0.5992, 0.5841,  ..., 0.5937, 0.5907, 0.5824],
#           [0.5982, 0.5745, 0.5600,  ..., 0.5968, 0.5895, 0.5959],
#           [0.5886, 0.5874, 0.5835,  ..., 0.5811, 0.5993, 0.5894]]]],
#        grad_fn=<SigmoidBackward0>)
# Max pool2d
# tensor([[[[0.5384, 0.5260, 0.5422,  ..., 0.5518, 0.5227, 0.5352],
#           [0.5584, 0.5201, 0.5420,  ..., 0.5428, 0.5275, 0.5358],
#           [0.5338, 0.5338, 0.5391,  ..., 0.5180, 0.5299, 0.5312],
#           ...,
#           [0.5517, 0.5321, 0.5489,  ..., 0.5335, 0.5538, 0.5289],
#           [0.5451, 0.5356, 0.5220,  ..., 0.5420, 0.5203, 0.5465],
#           [0.5244, 0.5454, 0.5414,  ..., 0.5251, 0.5395, 0.5427]],

#          [[0.5988, 0.5932, 0.5980,  ..., 0.5999, 0.6033, 0.5977],
#           [0.6062, 0.5864, 0.5956,  ..., 0.6110, 0.6072, 0.5951],
#           [0.5910, 0.5875, 0.5863,  ..., 0.5932, 0.6003, 0.6056],
#           ...,
#           [0.5994, 0.5990, 0.5958,  ..., 0.6021, 0.5861, 0.5960],
#           [0.6158, 0.5914, 0.5967,  ..., 0.5943, 0.5949, 0.5881],
#           [0.5978, 0.5824, 0.6011,  ..., 0.5847, 0.5965, 0.6153]],

#          [[0.5071, 0.5014, 0.4964,  ..., 0.4974, 0.5008, 0.5027],
#           [0.5165, 0.5324, 0.5047,  ..., 0.5093, 0.5074, 0.5094],
#           [0.5094, 0.5022, 0.5029,  ..., 0.4981, 0.5084, 0.5024],
#           ...,
#           [0.5109, 0.5001, 0.4953,  ..., 0.5007, 0.5216, 0.5146],
#           [0.4977, 0.5135, 0.4978,  ..., 0.5011, 0.5121, 0.5059],
#           [0.4965, 0.5121, 0.5340,  ..., 0.5139, 0.5097, 0.4952]],

#          ...,

#          [[0.4903, 0.4855, 0.4914,  ..., 0.4828, 0.4826, 0.4790],
#           [0.4731, 0.4696, 0.4760,  ..., 0.4823, 0.4808, 0.4837],
#           [0.4828, 0.4652, 0.4844,  ..., 0.4776, 0.4749, 0.4821],
#           ...,
#           [0.4843, 0.4943, 0.4909,  ..., 0.4829, 0.4901, 0.4767],
#           [0.4779, 0.4766, 0.4856,  ..., 0.4844, 0.4790, 0.4822],
#           [0.4805, 0.4845, 0.4744,  ..., 0.4744, 0.4770, 0.4814]],

#          [[0.5200, 0.5151, 0.5119,  ..., 0.4994, 0.5153, 0.5057],
#           [0.5175, 0.4960, 0.5265,  ..., 0.4978, 0.5069, 0.5099],
#           [0.5005, 0.5148, 0.4946,  ..., 0.5176, 0.5149, 0.5122],
#           ...,
#           [0.5206, 0.5155, 0.4883,  ..., 0.5135, 0.5111, 0.5145],
#           [0.4940, 0.5045, 0.5277,  ..., 0.5068, 0.5179, 0.5009],
#           [0.5097, 0.5223, 0.5220,  ..., 0.5156, 0.5135, 0.5034]],

#          [[0.5961, 0.5942, 0.5959,  ..., 0.6033, 0.5990, 0.5905],
#           [0.6020, 0.5947, 0.5964,  ..., 0.6050, 0.5955, 0.5892],
#           [0.5921, 0.5876, 0.5876,  ..., 0.5898, 0.6003, 0.6027],
#           ...,
#           [0.5902, 0.5947, 0.5780,  ..., 0.5959, 0.5944, 0.6017],
#           [0.5992, 0.5865, 0.6026,  ..., 0.6117, 0.5978, 0.5914],
#           [0.5982, 0.5939, 0.5877,  ..., 0.5884, 0.5968, 0.5993]]]],
#        grad_fn=<MaxPool2DWithIndicesBackward0>)
# Pass the dropout1
# tensor([[[[0.7179, 0.7013, 0.7229,  ..., 0.7357, 0.6969, 0.7136],
#           [0.7445, 0.6935, 0.7227,  ..., 0.7237, 0.7033, 0.7144],
#           [0.7118, 0.7117, 0.7187,  ..., 0.6907, 0.7065, 0.7082],
#           ...,
#           [0.7356, 0.7095, 0.7318,  ..., 0.7113, 0.7384, 0.7052],
#           [0.7268, 0.7142, 0.6961,  ..., 0.7227, 0.6938, 0.7286],
#           [0.6992, 0.7273, 0.7219,  ..., 0.7001, 0.7193, 0.7236]],

#          [[0.7984, 0.7910, 0.7973,  ..., 0.7999, 0.8044, 0.7969],
#           [0.8083, 0.7819, 0.7942,  ..., 0.8147, 0.8095, 0.7934],
#           [0.7880, 0.7834, 0.7817,  ..., 0.7909, 0.8004, 0.8074],
#           ...,
#           [0.7991, 0.7987, 0.7944,  ..., 0.8028, 0.7815, 0.7946],
#           [0.8210, 0.7885, 0.7956,  ..., 0.7924, 0.7932, 0.7841],
#           [0.7971, 0.7765, 0.8015,  ..., 0.7796, 0.7954, 0.8203]],

#          [[0.6761, 0.6685, 0.6619,  ..., 0.6632, 0.6677, 0.6703],
#           [0.6887, 0.7099, 0.6730,  ..., 0.6790, 0.6766, 0.6792],
#           [0.6792, 0.6696, 0.6705,  ..., 0.6641, 0.6778, 0.6699],
#           ...,
#           [0.6812, 0.6669, 0.6604,  ..., 0.6676, 0.6954, 0.6862],
#           [0.6636, 0.6847, 0.6638,  ..., 0.6682, 0.6828, 0.6746],
#           [0.6621, 0.6828, 0.7120,  ..., 0.6852, 0.6797, 0.6603]],

#          ...,

#          [[0.6538, 0.6473, 0.6553,  ..., 0.6438, 0.6435, 0.6387],
#           [0.6308, 0.6261, 0.6347,  ..., 0.6430, 0.6411, 0.6449],
#           [0.6437, 0.6202, 0.6458,  ..., 0.6369, 0.6333, 0.6427],
#           ...,
#           [0.6458, 0.6591, 0.6546,  ..., 0.6438, 0.6534, 0.6356],
#           [0.6372, 0.6355, 0.6474,  ..., 0.6458, 0.6386, 0.6430],
#           [0.6406, 0.6460, 0.6326,  ..., 0.6325, 0.6360, 0.6418]],

#          [[0.6933, 0.6868, 0.6825,  ..., 0.6659, 0.6870, 0.6743],
#           [0.6900, 0.6613, 0.7020,  ..., 0.6637, 0.6759, 0.6799],
#           [0.6673, 0.6864, 0.6594,  ..., 0.6901, 0.6866, 0.6829],
#           ...,
#           [0.6941, 0.6873, 0.6511,  ..., 0.6847, 0.6814, 0.6860],
#           [0.6587, 0.6727, 0.7036,  ..., 0.6758, 0.6905, 0.6679],
#           [0.6796, 0.6963, 0.6960,  ..., 0.6875, 0.6847, 0.6712]],

#          [[0.7948, 0.7922, 0.7946,  ..., 0.8044, 0.7987, 0.7874],
#           [0.8026, 0.7930, 0.7952,  ..., 0.8067, 0.7940, 0.7856],
#           [0.7895, 0.7835, 0.7835,  ..., 0.7864, 0.8003, 0.8035],
#           ...,
#           [0.7869, 0.7929, 0.7707,  ..., 0.7946, 0.7925, 0.8022],
#           [0.7990, 0.7819, 0.8034,  ..., 0.8156, 0.7971, 0.7886],
#           [0.7976, 0.7919, 0.7836,  ..., 0.7845, 0.7957, 0.7991]]]],
#        grad_fn=<MulBackward0>)
# Make flatten
# tensor([[0.7179, 0.7013, 0.7229,  ..., 0.7845, 0.7957, 0.7991]],
#        grad_fn=<ViewBackward0>)
# Pass the fc1
# tensor([[-0.0800, -0.4898, -0.2107,  0.0540, -0.4639,  0.1249,  0.0460,  0.3323,
#           0.1752,  0.3113,  0.2755, -0.3551, -0.1847, -0.1820,  0.0529,  0.1877,
#           0.0871, -0.5272,  0.2862, -0.1243, -0.1403, -0.4308, -0.6868, -0.5690,
#           0.2169,  0.0840, -0.0527,  0.1551,  0.1149,  0.5060, -0.1758, -0.0998,
#          -0.0066, -0.2974, -0.0020, -0.3446,  0.0612, -0.0914,  0.4873, -0.4425,
#          -0.5215,  0.3564, -0.0343,  0.2214, -0.1214,  0.0289, -0.4071,  0.0788,
#           0.2387, -0.4402,  0.0742, -0.1871,  0.3565, -0.4455, -0.0912, -0.4078,
#           0.5883, -0.3685,  0.1483,  0.4444, -0.2509, -0.0964, -0.4015, -0.1046,
#          -0.1057,  0.0153, -0.6300, -0.1157,  0.2781,  0.1869,  0.1470, -0.6784,
#           0.2090, -0.6118, -0.0329, -0.1991,  0.0863, -0.4613, -0.3167,  0.2874,
#          -0.0708,  0.4929,  0.2444, -0.2682,  0.1973, -0.8959,  0.0282,  0.0447,
#           0.0704, -0.0983, -0.2575, -0.3292, -0.4082, -0.1532, -0.0823, -0.1780,
#          -0.1260, -0.8865, -0.0148, -0.2691, -0.4960,  0.6533,  0.0324, -0.1439,
#           0.5283,  0.1840, -0.0748, -0.3664,  0.0989, -0.2214, -0.0372,  0.1012,
#          -0.5907,  0.0101,  0.4673, -0.3968, -0.4631,  0.6724, -0.2662,  0.1923,
#           0.7411, -0.0128, -0.4372, -0.6578, -0.1025,  0.0847,  0.0627,  0.2737]],
#        grad_fn=<AddmmBackward0>)
# Activation Function: sigmoid
# tensor([[-0.0769, -0.3872, -0.1900,  0.0540, -0.3712,  0.1249,  0.0460,  0.3323,
#           0.1752,  0.3113,  0.2755, -0.2989, -0.1687, -0.1664,  0.0529,  0.1877,
#           0.0871, -0.4098,  0.2862, -0.1168, -0.1309, -0.3500, -0.4968, -0.4339,
#           0.2169,  0.0840, -0.0513,  0.1551,  0.1149,  0.5060, -0.1612, -0.0950,
#          -0.0065, -0.2572, -0.0020, -0.2915,  0.0612, -0.0874,  0.4873, -0.3576,
#          -0.4064,  0.3564, -0.0337,  0.2214, -0.1143,  0.0289, -0.3344,  0.0788,
#           0.2387, -0.3561,  0.0742, -0.1706,  0.3565, -0.3595, -0.0871, -0.3349,
#           0.5883, -0.3082,  0.1483,  0.4444, -0.2219, -0.0919, -0.3307, -0.0993,
#          -0.1003,  0.0153, -0.4674, -0.1092,  0.2781,  0.1869,  0.1470, -0.4926,
#           0.2090, -0.4576, -0.0324, -0.1805,  0.0863, -0.3696, -0.2714,  0.2874,
#          -0.0684,  0.4929,  0.2444, -0.2352,  0.1973, -0.5918,  0.0282,  0.0447,
#           0.0704, -0.0936, -0.2270, -0.2805, -0.3352, -0.1420, -0.0790, -0.1630,
#          -0.1184, -0.5879, -0.0147, -0.2360, -0.3910,  0.6533,  0.0324, -0.1341,
#           0.5283,  0.1840, -0.0721, -0.3067,  0.0989, -0.1986, -0.0365,  0.1012,
#          -0.4461,  0.0101,  0.4673, -0.3276, -0.3706,  0.6724, -0.2337,  0.1923,
#           0.7411, -0.0127, -0.3542, -0.4820, -0.0974,  0.0847,  0.0627,  0.2737]],
#        grad_fn=<EluBackward0>)
# Activation Function: leaky_relu
# tensor([[-7.6917e-04, -3.8724e-03, -1.8996e-03,  5.3984e-02, -3.7117e-03,
#           1.2488e-01,  4.5962e-02,  3.3233e-01,  1.7524e-01,  3.1127e-01,
#           2.7547e-01, -2.9893e-03, -1.6867e-03, -1.6636e-03,  5.2947e-02,
#           1.8767e-01,  8.7112e-02, -4.0977e-03,  2.8620e-01, -1.1684e-03,
#          -1.3093e-03, -3.5002e-03, -4.9683e-03, -4.3390e-03,  2.1689e-01,
#           8.3957e-02, -5.1311e-04,  1.5508e-01,  1.1490e-01,  5.0601e-01,
#          -1.6121e-03, -9.5010e-04, -6.5295e-05, -2.5725e-03, -1.9689e-05,
#          -2.9149e-03,  6.1174e-02, -8.7352e-04,  4.8731e-01, -3.5759e-03,
#          -4.0639e-03,  3.5638e-01, -3.3726e-04,  2.2140e-01, -1.1434e-03,
#           2.8873e-02, -3.3443e-03,  7.8808e-02,  2.3869e-01, -3.5609e-03,
#           7.4169e-02, -1.7061e-03,  3.5653e-01, -3.5952e-03, -8.7127e-04,
#          -3.3490e-03,  5.8833e-01, -3.0825e-03,  1.4827e-01,  4.4443e-01,
#          -2.2192e-03, -9.1888e-04, -3.3067e-03, -9.9280e-04, -1.0029e-03,
#           1.5301e-02, -4.6739e-03, -1.0924e-03,  2.7812e-01,  1.8686e-01,
#           1.4703e-01, -4.9258e-03,  2.0899e-01, -4.5761e-03, -3.2351e-04,
#          -1.8052e-03,  8.6278e-02, -3.6956e-03, -2.7145e-03,  2.8743e-01,
#          -6.8362e-04,  4.9288e-01,  2.4443e-01, -2.3524e-03,  1.9731e-01,
#          -5.9177e-03,  2.8248e-02,  4.4734e-02,  7.0447e-02, -9.3634e-04,
#          -2.2698e-03, -2.8049e-03, -3.3518e-03, -1.4204e-03, -7.9031e-04,
#          -1.6304e-03, -1.1839e-03, -5.8791e-03, -1.4687e-04, -2.3595e-03,
#          -3.9101e-03,  6.5328e-01,  3.2375e-02, -1.3405e-03,  5.2830e-01,
#           1.8397e-01, -7.2083e-04, -3.0674e-03,  9.8876e-02, -1.9862e-03,
#          -3.6517e-04,  1.0119e-01, -4.4606e-03,  1.0119e-02,  4.6733e-01,
#          -3.2757e-03, -3.7065e-03,  6.7240e-01, -2.3371e-03,  1.9227e-01,
#           7.4107e-01, -1.2717e-04, -3.5416e-03, -4.8200e-03, -9.7413e-04,
#           8.4676e-02,  6.2700e-02,  2.7372e-01]], grad_fn=<LeakyReluBackward0>)
# Pass the fc2
# tensor([[-0.0115,  0.0056,  0.1435,  0.0355,  0.0255, -0.0307,  0.1350, -0.0782,
#           0.0028,  0.1138]], grad_fn=<AddmmBackward0>)
# Activation Function: softmax
# tensor([[-0.0115,  0.0056,  0.1435,  0.0355,  0.0255, -0.0307,  0.1350, -0.0782,
#           0.0028,  0.1138]], grad_fn=<AddmmBackward0>)
# tensor([[-2.3507, -2.3336, -2.1957, -2.3037, -2.3137, -2.3699, -2.2042, -2.4174,
#          -2.3364, -2.2254]], grad_fn=<LogSoftmaxBackward0>)
