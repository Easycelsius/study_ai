{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cdb13b-1a18-4f58-9b34-df013de46baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://yhkim4504.tistory.com/5\n",
    "# https://yhkim4504.tistory.com/6\n",
    "# https://yhkim4504.tistory.com/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5974a15-b4c5-45f0-bfde-c7de6f75408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eea105f0-5d4f-4ab3-9be1-70cd87886608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "def single_image_show(tensor):\n",
    "    plt.imshow(tensor.permute(1, 2, 0), vmin=0, vmax=255)\n",
    "\n",
    "# 786\n",
    "def channels_plt(tensor, row=16, col=48):\n",
    "    fig, axs = plt.subplots(row, col, figsize=(32, 48))\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        ax.imshow(tensor[0, i, :].detach().cpu().numpy())\n",
    "        ax.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a025e96-eb97-40b5-b728-a20dbc1351c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 512, 512])\n",
      "torch.Size([10, 768, 32, 32])\n",
      "torch.Size([10, 1024, 768])\n",
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1025, 768])\n",
      "torch.Size([10, 1, 768])\n",
      "torch.Size([10, 1025, 768])\n",
      "torch.Size([10, 1025, 768])\n"
     ]
    }
   ],
   "source": [
    "# Patch Embedding\n",
    "\n",
    "img_size = (512, 512)\n",
    "\n",
    "# 배치사이즈10, 채널3, 높이512, 넓이512\n",
    "x = torch.randn(10, 3, img_size[0], img_size[1])\n",
    "print(x.shape) # torch.Size([10, 3, 512, 512])\n",
    "\n",
    "# BATCHxCxH×W 형태를 가진 이미지 --> BATCHxNx(P*P*C)의 벡터로 임베딩: P는 패치사이즈이며 N은 패치의 개수(H*W / (P*P))\n",
    "patch_size = 16 #16pixels\n",
    "in_channels = 3\n",
    "emb_size = 768\n",
    "\n",
    "# print(nn.Sequential(\n",
    "#     nn.Conv2d(in_channels=in_channels, out_channels=emb_size, kernel_size=3, stride=1, padding=1),\n",
    "# )(x).shape) # torch.Size([10, 768, 512, 512])\n",
    "\n",
    "projection = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=in_channels, out_channels=emb_size, kernel_size=patch_size, stride=patch_size),\n",
    ")\n",
    "print(projection(x).shape) # torch.Size([10, 768, 32, 32])\n",
    "\n",
    "p_size = projection(x).shape\n",
    "b = p_size[0]\n",
    "c = p_size[1]\n",
    "h = p_size[2]\n",
    "w = p_size[3]\n",
    "\n",
    "projection = projection(x).flatten(2).transpose(1,2)\n",
    "# projection = projection(x).view(b, h * w, c) # wrong\n",
    "print(projection.shape) # torch.Size([10, 1024, 768])\n",
    "\n",
    "# cls_token, pos encoding Parameter 정의\n",
    "cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "positions = nn.Parameter(torch.randn((img_size[0] // patch_size)**2 + 1, emb_size))\n",
    "print(cls_token.shape) # torch.Size([1, 1, 768])\n",
    "print(positions.shape) # torch.Size([1025, 768])\n",
    "\n",
    "# batch 사이즈만큼 cls_token을 반복하여 크기를 맞춤\n",
    "cls_tokens = cls_token.repeat(b, 1, 1)\n",
    "print(cls_tokens.shape) # torch.Size([10, 1, 768])\n",
    "\n",
    "# 배치 다음의 1차원 기준으로 cls_tokens, projection을 concat\n",
    "cat_x = torch.cat([cls_tokens, projection], dim=1) # torch.Size([10, 1, 768]), torch.Size([10, 1024, 768])\n",
    "print(cat_x.shape) # torch.Size([10, 1025, 768])\n",
    "\n",
    "# position encoding을 더해줌\n",
    "cat_x += positions # torch.Size([10, 1025, 768]) + torch.Size([1025, 768])\n",
    "print(cat_x.shape) # torch.Size([10, 1025, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03e7ccf1-570b-4a32-9d98-f3c29b7458ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 코드를 클래스로 구현한다면\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels:int=3, patch_size:int=16, emb_size:int=768, img_size:int=512):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "        self.positions = nn.Parameter(torch.randn((img_size // patch_size)**2 + 1, emb_size))\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.projection(x)\n",
    "        b, c, h, w = x.shape\n",
    "        x = x.flatten(2).transpose(2,1)\n",
    "        cls_token = nn.Parameter(self.cls_token)\n",
    "        cls_toekns = cls_token.repeat(b, 1, 1)\n",
    "        x = torch.cat([cls_toekns, x], dim=1)\n",
    "        x += self.positions\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a027a706-9c6c-4263-b97c-43d7917859dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1025, 768])\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "Linear(in_features=768, out_features=768, bias=True)\n",
      "torch.Size([10, 8, 1025, 96])\n",
      "torch.Size([10, 8, 1025, 96])\n",
      "torch.Size([10, 8, 1025, 96])\n",
      "energy : torch.Size([10, 8, 1025, 1025])\n",
      "att : torch.Size([10, 8, 1025, 1025])\n",
      "out : torch.Size([10, 8, 1025, 96])\n",
      "out2 :  torch.Size([10, 1025, 768])\n"
     ]
    }
   ],
   "source": [
    "# Multi Head Attention (MHA)\n",
    "emb_size = 768\n",
    "num_heads = 8\n",
    "\n",
    "x = PatchEmbedding()(x = torch.randn(10, 3, 512, 512))\n",
    "print(x.shape) # torch.Size([10, 1025, 768])\n",
    "\n",
    "# Linear Projection\n",
    "\n",
    "# VIT에서의 MHA는 QKV가 같은 3개의 텐서로 입력\n",
    "# 3개의 Linear Projection을 통해 임베딩된 후 여러 개의 Head로 나눠짐\n",
    "# 이후 각각 Scaled Dot-Product Attention을 진행\n",
    "keys = nn.Linear(emb_size, emb_size)\n",
    "queries = nn.Linear(emb_size, emb_size)\n",
    "values = nn.Linear(emb_size, emb_size)\n",
    "print(keys)\n",
    "print(queries)\n",
    "print(values)\n",
    "# Linear(in_features=768, out_features=768, bias=True) \n",
    "# Linear(in_features=768, out_features=768, bias=True) \n",
    "# Linear(in_features=768, out_features=768, bias=True)\n",
    "\n",
    "\n",
    "# Multi-Head\n",
    "\n",
    "# Linear Projection을 거친 QKV를 8개의 Multi-Head로 나누어줌\n",
    "keys = rearrange(keys(x), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "# keys = keys(x)\n",
    "# print(keys.shape) # torch.Size([10, 1025, 768])\n",
    "# b, hw, c = keys.shape\n",
    "# keys = keys.view(b, num_heads, hw, int(c/num_heads))\n",
    "print(keys.shape) # torch.Size([10, 8, 1025, 96])\n",
    "\n",
    "queries = rearrange(queries(x), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "# queries = queries(x)\n",
    "# b, hw, c = queries.shape\n",
    "# queries = queries.view(b, num_heads, hw, int(c/num_heads))\n",
    "print(queries.shape) # torch.Size([10, 8, 1025, 96])\n",
    "\n",
    "values = rearrange(values(x), \"b n (h d) -> b h n d\", h=num_heads)\n",
    "# values = values(x)\n",
    "# b, hw, c = values.shape\n",
    "# values = values.view(b, num_heads, hw, int(c/num_heads))\n",
    "print(values.shape) # torch.Size([10, 8, 1025, 96])\n",
    "\n",
    "# Queries * Keys\n",
    "energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "print('energy :', energy.shape) # torch.Size([10, 8, 1025, 1025])\n",
    "\n",
    "# Get Attention Score\n",
    "scaling = emb_size ** (1/2)\n",
    "att = F.softmax(energy, dim=-1) / scaling\n",
    "print('att :', att.shape) # torch.Size([10, 8, 1025, 1025])\n",
    "\n",
    "# Attention Score * values\n",
    "out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "print('out :', out.shape) # torch.Size([10, 8, 1025, 96])\n",
    "\n",
    "# Rearrage to emb_size\n",
    "b, h, n, d = out.shape\n",
    "out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "print('out2 : ', out.shape) # torch.Size([10, 1025, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e68be06-3fb5-4f34-aff7-c5c42897dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위 코드를 클래스로 구현한다면\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int = 768, num_heads: int = 8, dropout: float = 0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        # fuse the queries, keys and values in one matrix\n",
    "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "        \n",
    "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        # sum up over the last axis\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "            \n",
    "        scaling = self.emb_size ** (1/2)\n",
    "        att = F.softmax(energy, dim=-1) / scaling\n",
    "        att = self.att_drop(att)\n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "\n",
    "# QKV 당 각각 1개씩의 Linear Layer를 적용한 것을 텐서 연산을 한번에 하기 위해 Linear Layer를 emb_size*3으로 설정한 후 \n",
    "# 연산시 QKV를 각각 나눠주게 됩니다. 또한 Attention 시 무시할 정보를 설정하기 위한 masking 코드도 추가\n",
    "# 마지막으로 나오는 out은 최종적으로 한번의 Linear Layer를 거쳐서 나오게 되는게 MHA의 모든 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d808e8d7-592f-4259-8328-a498db44a8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIT Encoder 구조\n",
    "\n",
    "# 나중에 fn을 입력받아 fn의 forward 후 res를 더해 사용\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "# MHA 이후에 진행되는 MLP 부분\n",
    "# inear - GELU - Dropout - Linear 순으로 진행\n",
    "# 두개의 Linear 레이어가 있는 것을 확인할 수 있으며 \n",
    "# 첫번째 레이어에서는 expansion을 곱해준 만큼 임베딩 사이즈를 확장하고 GELU와 Dropout 후에 \n",
    "# 두번째 Linear 레이어에서 다시 원래의 emb_size로 축소\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "# ResidualAdd와 FeedForwardBlock의 구현체\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size: int = 768,\n",
    "                 drop_p: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0185fdc7-954f-4bc2-a502-c4307c36a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 3, 512, 512) # 배치사이즈10, 채널3, 높이512, 넓이512\n",
    "patches_embedded = PatchEmbedding()(x)\n",
    "TransformerEncoderBlock()(patches_embedded).shape # torch.Size([10, 1025, 768])\n",
    "\n",
    "# Block 쌓기\n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n",
    "\n",
    "# Head\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size: int = 768, n_classes: int = 1000):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size), \n",
    "            nn.Linear(emb_size, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42a03d16-db09-4cfa-abd0-5433e55ab6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─PatchEmbedding: 1-1                    [-1, 197, 768]            --\n",
      "|    └─Sequential: 2-1                   [-1, 768, 14, 14]         --\n",
      "|    |    └─Conv2d: 3-1                  [-1, 768, 14, 14]         590,592\n",
      "├─TransformerEncoder: 1-2                [-1, 197, 768]            --\n",
      "|    └─TransformerEncoderBlock: 2-2      [-1, 197, 768]            --\n",
      "|    |    └─ResidualAdd: 3-2             [-1, 197, 768]            2,363,904\n",
      "|    |    └─ResidualAdd: 3-3             [-1, 197, 768]            4,723,968\n",
      "|    └─TransformerEncoderBlock: 2-3      [-1, 197, 768]            --\n",
      "|    |    └─ResidualAdd: 3-4             [-1, 197, 768]            2,363,904\n",
      "|    |    └─ResidualAdd: 3-5             [-1, 197, 768]            4,723,968\n",
      "|    └─TransformerEncoderBlock: 2-4      [-1, 197, 768]            --\n",
      "|    |    └─ResidualAdd: 3-6             [-1, 197, 768]            2,363,904\n",
      "|    |    └─ResidualAdd: 3-7             [-1, 197, 768]            4,723,968\n",
      "|    └─TransformerEncoderBlock: 2-5      [-1, 197, 768]            --\n",
      "|    |    └─ResidualAdd: 3-8             [-1, 197, 768]            2,363,904\n",
      "|    |    └─ResidualAdd: 3-9             [-1, 197, 768]            4,723,968\n",
      "|    └─TransformerEncoderBlock: 2-6      [-1, 197, 768]            --\n",
      "|    |    └─ResidualAdd: 3-10            [-1, 197, 768]            2,363,904\n",
      "|    |    └─ResidualAdd: 3-11            [-1, 197, 768]            4,723,968\n",
      "|    └─TransformerEncoderBlock: 2-7      [-1, 197, 768]            --\n",
      "|    |    └─ResidualAdd: 3-12            [-1, 197, 768]            2,363,904\n",
      "|    |    └─ResidualAdd: 3-13            [-1, 197, 768]            4,723,968\n",
      "|    └─TransformerEncoderBlock: 2-8      [-1, 197, 768]            --\n",
      "|    |    └─ResidualAdd: 3-14            [-1, 197, 768]            2,363,904\n",
      "|    |    └─ResidualAdd: 3-15            [-1, 197, 768]            4,723,968\n",
      "|    └─TransformerEncoderBlock: 2-9      [-1, 197, 768]            --\n",
      "|    |    └─ResidualAdd: 3-16            [-1, 197, 768]            2,363,904\n",
      "|    |    └─ResidualAdd: 3-17            [-1, 197, 768]            4,723,968\n",
      "|    └─TransformerEncoderBlock: 2-10     [-1, 197, 768]            --\n",
      "|    |    └─ResidualAdd: 3-18            [-1, 197, 768]            2,363,904\n",
      "|    |    └─ResidualAdd: 3-19            [-1, 197, 768]            4,723,968\n",
      "|    └─TransformerEncoderBlock: 2-11     [-1, 197, 768]            --\n",
      "|    |    └─ResidualAdd: 3-20            [-1, 197, 768]            2,363,904\n",
      "|    |    └─ResidualAdd: 3-21            [-1, 197, 768]            4,723,968\n",
      "|    └─TransformerEncoderBlock: 2-12     [-1, 197, 768]            --\n",
      "|    |    └─ResidualAdd: 3-22            [-1, 197, 768]            2,363,904\n",
      "|    |    └─ResidualAdd: 3-23            [-1, 197, 768]            4,723,968\n",
      "|    └─TransformerEncoderBlock: 2-13     [-1, 197, 768]            --\n",
      "|    |    └─ResidualAdd: 3-24            [-1, 197, 768]            2,363,904\n",
      "|    |    └─ResidualAdd: 3-25            [-1, 197, 768]            4,723,968\n",
      "├─ClassificationHead: 1-3                [-1, 1000]                --\n",
      "|    └─Reduce: 2-14                      [-1, 768]                 --\n",
      "|    └─LayerNorm: 2-15                   [-1, 768]                 1,536\n",
      "|    └─Linear: 2-16                      [-1, 1000]                769,000\n",
      "==========================================================================================\n",
      "Total params: 86,415,592\n",
      "Trainable params: 86,415,592\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 458.14\n",
      "==========================================================================================\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 1.16\n",
      "Params size (MB): 329.65\n",
      "Estimated Total Size (MB): 331.39\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─PatchEmbedding: 1-1                    [-1, 197, 768]            --\n",
       "|    └─Sequential: 2-1                   [-1, 768, 14, 14]         --\n",
       "|    |    └─Conv2d: 3-1                  [-1, 768, 14, 14]         590,592\n",
       "├─TransformerEncoder: 1-2                [-1, 197, 768]            --\n",
       "|    └─TransformerEncoderBlock: 2-2      [-1, 197, 768]            --\n",
       "|    |    └─ResidualAdd: 3-2             [-1, 197, 768]            2,363,904\n",
       "|    |    └─ResidualAdd: 3-3             [-1, 197, 768]            4,723,968\n",
       "|    └─TransformerEncoderBlock: 2-3      [-1, 197, 768]            --\n",
       "|    |    └─ResidualAdd: 3-4             [-1, 197, 768]            2,363,904\n",
       "|    |    └─ResidualAdd: 3-5             [-1, 197, 768]            4,723,968\n",
       "|    └─TransformerEncoderBlock: 2-4      [-1, 197, 768]            --\n",
       "|    |    └─ResidualAdd: 3-6             [-1, 197, 768]            2,363,904\n",
       "|    |    └─ResidualAdd: 3-7             [-1, 197, 768]            4,723,968\n",
       "|    └─TransformerEncoderBlock: 2-5      [-1, 197, 768]            --\n",
       "|    |    └─ResidualAdd: 3-8             [-1, 197, 768]            2,363,904\n",
       "|    |    └─ResidualAdd: 3-9             [-1, 197, 768]            4,723,968\n",
       "|    └─TransformerEncoderBlock: 2-6      [-1, 197, 768]            --\n",
       "|    |    └─ResidualAdd: 3-10            [-1, 197, 768]            2,363,904\n",
       "|    |    └─ResidualAdd: 3-11            [-1, 197, 768]            4,723,968\n",
       "|    └─TransformerEncoderBlock: 2-7      [-1, 197, 768]            --\n",
       "|    |    └─ResidualAdd: 3-12            [-1, 197, 768]            2,363,904\n",
       "|    |    └─ResidualAdd: 3-13            [-1, 197, 768]            4,723,968\n",
       "|    └─TransformerEncoderBlock: 2-8      [-1, 197, 768]            --\n",
       "|    |    └─ResidualAdd: 3-14            [-1, 197, 768]            2,363,904\n",
       "|    |    └─ResidualAdd: 3-15            [-1, 197, 768]            4,723,968\n",
       "|    └─TransformerEncoderBlock: 2-9      [-1, 197, 768]            --\n",
       "|    |    └─ResidualAdd: 3-16            [-1, 197, 768]            2,363,904\n",
       "|    |    └─ResidualAdd: 3-17            [-1, 197, 768]            4,723,968\n",
       "|    └─TransformerEncoderBlock: 2-10     [-1, 197, 768]            --\n",
       "|    |    └─ResidualAdd: 3-18            [-1, 197, 768]            2,363,904\n",
       "|    |    └─ResidualAdd: 3-19            [-1, 197, 768]            4,723,968\n",
       "|    └─TransformerEncoderBlock: 2-11     [-1, 197, 768]            --\n",
       "|    |    └─ResidualAdd: 3-20            [-1, 197, 768]            2,363,904\n",
       "|    |    └─ResidualAdd: 3-21            [-1, 197, 768]            4,723,968\n",
       "|    └─TransformerEncoderBlock: 2-12     [-1, 197, 768]            --\n",
       "|    |    └─ResidualAdd: 3-22            [-1, 197, 768]            2,363,904\n",
       "|    |    └─ResidualAdd: 3-23            [-1, 197, 768]            4,723,968\n",
       "|    └─TransformerEncoderBlock: 2-13     [-1, 197, 768]            --\n",
       "|    |    └─ResidualAdd: 3-24            [-1, 197, 768]            2,363,904\n",
       "|    |    └─ResidualAdd: 3-25            [-1, 197, 768]            4,723,968\n",
       "├─ClassificationHead: 1-3                [-1, 1000]                --\n",
       "|    └─Reduce: 2-14                      [-1, 768]                 --\n",
       "|    └─LayerNorm: 2-15                   [-1, 768]                 1,536\n",
       "|    └─Linear: 2-16                      [-1, 1000]                769,000\n",
       "==========================================================================================\n",
       "Total params: 86,415,592\n",
       "Trainable params: 86,415,592\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 458.14\n",
       "==========================================================================================\n",
       "Input size (MB): 0.57\n",
       "Forward/backward pass size (MB): 1.16\n",
       "Params size (MB): 329.65\n",
       "Estimated Total Size (MB): 331.39\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ViT(nn.Sequential):\n",
    "    def __init__(self,     \n",
    "                in_channels: int = 3,\n",
    "                patch_size: int = 16,\n",
    "                emb_size: int = 768,\n",
    "                img_size: int = 224,\n",
    "                depth: int = 12,\n",
    "                n_classes: int = 1000,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )\n",
    "        \n",
    "summary(ViT(), (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c7b138-f599-4312-9c71-97073d19fcc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2887cca1-f2d1-488e-951e-767a7ecfd948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4de6a5-0a46-4d75-ae8b-498f3b0c9529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-ai-project-venv",
   "language": "python",
   "name": "my-ai-project-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
